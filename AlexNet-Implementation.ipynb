{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('labels.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "breed = set(df['breed'])\n",
    "n_class = len(breed)\n",
    "class_to_num = dict(zip(breed, range(n_class)))\n",
    "num_to_class = dict(zip(range(n_class), breed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 945/10222 [00:05<01:00, 153.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d08f407e6255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmn2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train/%s.jpg'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mn=[]\n",
    "mn1=[]\n",
    "mn2=[]\n",
    "for i in tqdm(range(n)):\n",
    "    img=cv2.imread('train/%s.jpg' % df['id'][i])\n",
    "    height, width, channels = img.shape\n",
    "    mn.append(height)\n",
    "    mn1.append(width)\n",
    "    mn2.append(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386.74721189591077, 443.33153981608297, 3.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mn)/len(mn),sum(mn1)/len(mn1),sum(mn2)/len(mn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [01:29<00:00, 114.29it/s]\n"
     ]
    }
   ],
   "source": [
    "width = 299\n",
    "X = np.zeros((n, width, width, 3), dtype=np.uint8)\n",
    "y = np.zeros((n, n_class), dtype=np.uint8)\n",
    "for i in tqdm(range(n)):\n",
    "    X[i] = cv2.resize(cv2.imread('train/%s.jpg' % df['id'][i]), (width, width))\n",
    "    y[i][class_to_num[df['breed'][i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a7d23870a550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[0][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(8):\n",
    "    random_index = random.randint(0, n-1)\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.imshow(X[random_index][:,:,::-1])\n",
    "    plt.title(num_to_class[y[random_index].argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "#from collections import defaultdict\n",
    "#from scipy import ndimage\n",
    "\n",
    "def flatten_tf_array(array):\n",
    "    shape = array.get_shape().as_list()\n",
    "    return tf.reshape(array, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "def one_hot_encode(np_array, num_labels):\n",
    "    return (np.arange(num_labels) == np_array[:,None]).astype(np.float32)\n",
    "\n",
    "def reformat_data(dataset, labels, image_width, image_height, image_depth):\n",
    "    np_dataset_ = np.array([np.array(image_data).reshape(image_width, image_height, image_depth) for image_data in dataset])\n",
    "    num_labels = len(np.unique(labels))\n",
    "    np_labels_ = one_hot_encode(np.array(labels, dtype=np.float32), num_labels)\n",
    "    np_dataset, np_labels = randomize(np_dataset_, np_labels_)\n",
    "    return np_dataset, np_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Oxford 17 category Flower Dataset, Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 60276736 / 60270631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Succesfully downloaded', '17flowers.tgz', 60270631, 'bytes.')\n",
      "File Extracted\n",
      "Starting to parse images...\n",
      "Parsing Done!\n",
      "Training set (1000, 224, 224, 3) (1000, 17)\n",
      "Test set (360, 224, 224, 3) (360, 17)\n"
     ]
    }
   ],
   "source": [
    "ox17_image_width = 224\n",
    "ox17_image_height = 224\n",
    "ox17_image_depth = 3\n",
    "ox17_num_labels = 17\n",
    "\n",
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "train_dataset_, train_labels_ = oxflower17.load_data(one_hot=True)\n",
    "train_dataset_ox17, train_labels_ox17 = train_dataset_[:1000,:,:,:], train_labels_[:1000,:]\n",
    "test_dataset_ox17, test_labels_ox17 = train_dataset_[1000:,:,:,:], train_labels_[1000:,:]\n",
    "\n",
    "print('Training set', train_dataset_ox17.shape, train_labels_ox17.shape)\n",
    "print('Test set', test_dataset_ox17.shape, test_labels_ox17.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALEX_PATCH_DEPTH_1, ALEX_PATCH_DEPTH_2, ALEX_PATCH_DEPTH_3, ALEX_PATCH_DEPTH_4 = 96, 256, 384, 256\n",
    "ALEX_PATCH_SIZE_1, ALEX_PATCH_SIZE_2, ALEX_PATCH_SIZE_3, ALEX_PATCH_SIZE_4 = 11, 5, 3, 3\n",
    "ALEX_NUM_HIDDEN_1, ALEX_NUM_HIDDEN_2 = 4096, 4096\n",
    " \n",
    " \n",
    "def variables_alexnet(patch_size1 = ALEX_PATCH_SIZE_1, patch_size2 = ALEX_PATCH_SIZE_2, \n",
    "                      patch_size3 = ALEX_PATCH_SIZE_3, patch_size4 = ALEX_PATCH_SIZE_4, \n",
    "                      patch_depth1 = ALEX_PATCH_DEPTH_1, patch_depth2 = ALEX_PATCH_DEPTH_2, \n",
    "                      patch_depth3 = ALEX_PATCH_DEPTH_3, patch_depth4 = ALEX_PATCH_DEPTH_4, \n",
    "                      num_hidden1 = ALEX_NUM_HIDDEN_1, num_hidden2 = ALEX_NUM_HIDDEN_2,\n",
    "                      image_width = 224, image_height = 224, image_depth = 3, num_labels = 17):\n",
    " \n",
    "    w1 = tf.Variable(tf.truncated_normal([patch_size1, patch_size1, image_depth, patch_depth1], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.zeros([patch_depth1]))\n",
    " \n",
    "    w2 = tf.Variable(tf.truncated_normal([patch_size2, patch_size2, patch_depth1, patch_depth2], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.constant(1.0, shape=[patch_depth2]))\n",
    " \n",
    "    w3 = tf.Variable(tf.truncated_normal([patch_size3, patch_size3, patch_depth2, patch_depth3], stddev=0.1))\n",
    "    b3 = tf.Variable(tf.zeros([patch_depth3]))\n",
    " \n",
    "    w4 = tf.Variable(tf.truncated_normal([patch_size4, patch_size4, patch_depth3, patch_depth3], stddev=0.1))\n",
    "    b4 = tf.Variable(tf.constant(1.0, shape=[patch_depth3]))\n",
    " \n",
    "    w5 = tf.Variable(tf.truncated_normal([patch_size4, patch_size4, patch_depth3, patch_depth3], stddev=0.1))\n",
    "    b5 = tf.Variable(tf.zeros([patch_depth3]))\n",
    " \n",
    "    pool_reductions = 3\n",
    "    conv_reductions = 2\n",
    "    no_reductions = pool_reductions + conv_reductions\n",
    "    w6 = tf.Variable(tf.truncated_normal([(image_width // 2**no_reductions)*(image_height // 2**no_reductions)*patch_depth3, num_hidden1], stddev=0.1))\n",
    "    b6 = tf.Variable(tf.constant(1.0, shape = [num_hidden1]))\n",
    " \n",
    "    w7 = tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2], stddev=0.1))\n",
    "    b7 = tf.Variable(tf.constant(1.0, shape = [num_hidden2]))\n",
    " \n",
    "    w8 = tf.Variable(tf.truncated_normal([num_hidden2, num_labels], stddev=0.1))\n",
    "    b8 = tf.Variable(tf.constant(1.0, shape = [num_labels]))\n",
    " \n",
    "    variables = {\n",
    "                 'w1': w1, 'w2': w2, 'w3': w3, 'w4': w4, 'w5': w5, 'w6': w6, 'w7': w7, 'w8': w8, \n",
    "                 'b1': b1, 'b2': b2, 'b3': b3, 'b4': b4, 'b5': b5, 'b6': b6, 'b7': b7, 'b8': b8\n",
    "                }\n",
    "    return variables\n",
    " \n",
    " \n",
    "def model_alexnet(data, variables):\n",
    "    layer1_conv = tf.nn.conv2d(data, variables['w1'], [1, 4, 4, 1], padding='SAME')\n",
    "    layer1_relu = tf.nn.relu(layer1_conv + variables['b1'])\n",
    "    layer1_pool = tf.nn.max_pool(layer1_relu, [1, 3, 3, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    layer1_norm = tf.nn.local_response_normalization(layer1_pool)\n",
    " \n",
    "    layer2_conv = tf.nn.conv2d(layer1_norm, variables['w2'], [1, 1, 1, 1], padding='SAME')\n",
    "    layer2_relu = tf.nn.relu(layer2_conv + variables['b2'])\n",
    "    layer2_pool = tf.nn.max_pool(layer2_relu, [1, 3, 3, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    layer2_norm = tf.nn.local_response_normalization(layer2_pool)\n",
    " \n",
    "    layer3_conv = tf.nn.conv2d(layer2_norm, variables['w3'], [1, 1, 1, 1], padding='SAME')\n",
    "    layer3_relu = tf.nn.relu(layer3_conv + variables['b3'])\n",
    " \n",
    "    layer4_conv = tf.nn.conv2d(layer3_relu, variables['w4'], [1, 1, 1, 1], padding='SAME')\n",
    "    layer4_relu = tf.nn.relu(layer4_conv + variables['b4'])\n",
    " \n",
    "    layer5_conv = tf.nn.conv2d(layer4_relu, variables['w5'], [1, 1, 1, 1], padding='SAME')\n",
    "    layer5_relu = tf.nn.relu(layer5_conv + variables['b5'])\n",
    "    layer5_pool = tf.nn.max_pool(layer4_relu, [1, 3, 3, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    layer5_norm = tf.nn.local_response_normalization(layer5_pool)\n",
    " \n",
    "    flat_layer = flatten_tf_array(layer5_norm)\n",
    "    layer6_fccd = tf.matmul(flat_layer, variables['w6']) + variables['b6']\n",
    "    layer6_tanh = tf.tanh(layer6_fccd)\n",
    "    layer6_drop = tf.nn.dropout(layer6_tanh, 0.5)\n",
    " \n",
    "    layer7_fccd = tf.matmul(layer6_drop, variables['w7']) + variables['b7']\n",
    "    layer7_tanh = tf.tanh(layer7_fccd)\n",
    "    layer7_drop = tf.nn.dropout(layer7_tanh, 0.5)\n",
    " \n",
    "    logits = tf.matmul(layer7_drop, variables['w8']) + variables['b8']\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with learning_rate 0.01\n",
      "step 0000 : loss is 013.47, accuracy on training set 3.12 %, accuracy on test set 4.44 %\n",
      "step 0010 : loss is 038.79, accuracy on training set 9.38 %, accuracy on test set 4.44 %\n",
      "step 0020 : loss is 023.62, accuracy on training set 0.00 %, accuracy on test set 4.72 %\n",
      "step 0030 : loss is 020.23, accuracy on training set 0.00 %, accuracy on test set 6.11 %\n",
      "step 0040 : loss is 015.01, accuracy on training set 6.25 %, accuracy on test set 6.94 %\n",
      "step 0050 : loss is 020.62, accuracy on training set 6.25 %, accuracy on test set 5.28 %\n",
      "step 0060 : loss is 014.14, accuracy on training set 9.38 %, accuracy on test set 6.67 %\n",
      "step 0070 : loss is 015.22, accuracy on training set 6.25 %, accuracy on test set 5.28 %\n",
      "step 0080 : loss is 013.26, accuracy on training set 6.25 %, accuracy on test set 5.83 %\n",
      "step 0090 : loss is 015.33, accuracy on training set 3.12 %, accuracy on test set 5.28 %\n",
      "step 0100 : loss is 015.77, accuracy on training set 15.62 %, accuracy on test set 5.56 %\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#from cnn_models import lenet5, lenet5_like, alexnet, vggnet16\n",
    "#from utils import *\n",
    "\n",
    "#Variables used in the constructing and running the graph\n",
    "num_steps = 10001\n",
    "display_step = 100\n",
    "#learning_rate = 0.5\n",
    "batch_size = 32\n",
    "learning_rate=0.01\n",
    "\n",
    "#General\n",
    "image_width = ox17_image_width\n",
    "image_height = ox17_image_height\n",
    "image_depth = ox17_image_depth\n",
    "num_labels = ox17_num_labels\n",
    "\n",
    "test_dataset = test_dataset_ox17\n",
    "test_labels = test_labels_ox17\n",
    "train_dataset = train_dataset_ox17\n",
    "train_labels = train_labels_ox17\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #1) First we put the input data in a tensorflow friendly form. \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_test_dataset = tf.constant(test_dataset, tf.float32)\n",
    "\n",
    "    #2) Then, the weight matrices and bias vectors are initialized\n",
    "    #variables_ = variables_lenet5()\n",
    "    #variables = variables_lenet5_like()\n",
    "    variables = variables_alexnet()\n",
    "    #variables = variables_vggnet16()\n",
    "    variables = variables_alexnet(image_width = image_width, image_height=image_height, image_depth = image_depth, num_labels = num_labels)\n",
    "\n",
    "\n",
    "    #3. The model used to calculate the logits (predicted labels)\n",
    "    #model = model_lenet5\n",
    "    #model = model_lenet5_like\n",
    "    model = model_alexnet\n",
    "    #model = model_vggnet16\n",
    "    logits = model(tf_train_dataset, variables)\n",
    "\n",
    "    #4. then we compute the softmax cross entropy between the logits and the (actual) labels\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "\n",
    "    #5. The optimizer is used to calculate the gradients of the loss function \n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.0).minimize(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset, variables))\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized with learning_rate', learning_rate)\n",
    "    for step in range(num_steps):\n",
    "        #Since we are using stochastic gradient descent, we are selecting  small batches from the training dataset,\n",
    "        #and training the convolutional neural network each time with a batch. \n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        train_accuracy = accuracy(predictions, batch_labels)\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "            message = \"step {:04d} : loss is {:06.2f}, accuracy on training set {:02.2f} %, accuracy on test set {:02.2f} %\".format(step, l, train_accuracy, test_accuracy)\n",
    "            print(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
